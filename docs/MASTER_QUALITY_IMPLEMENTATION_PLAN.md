# Master-Crafted Quality Implementation Plan
**AI Worldbuilding System: Path to Sanderson/Tolkien-Level Quality**

**Version**: 1.0
**Date**: 2025-10-13
**Goal**: Transform current system into a Replit-like tool that produces master-crafted worldbuilding across all genres

---

## Executive Summary

### Current State
- **System**: Prompt engineering + Constitutional AI (self-critique loop)
- **Quality**: 7.0-7.5/10 average, 40-60% clich√© reduction
- **Sanderson Alignment**: 6.7/10
- **Genre Support**: Fantasy-focused, limited flexibility

### Target State
- **Quality**: 9.0-9.5/10 average, 90-98% clich√© reduction
- **Sanderson Alignment**: 8.5-9.0/10
- **Genre Support**: Universal (fantasy, sci-fi, horror, contemporary, experimental)
- **UX**: Replit-like (zero friction, conversational, instant results)

### Path Forward
**3 Phases**: Quick Wins (1 week) ‚Üí Multi-Agent Critics (3-6 weeks) ‚Üí RAG Examples (3-4 weeks)

**Expected Total Improvement**: +120-170% quality over current baseline

---

## Table of Contents

1. [Research Findings Summary](#research-findings-summary)
2. [Phase 1: Quick Wins (1 Week)](#phase-1-quick-wins-1-week)
3. [Phase 2: Multi-Agent Critics (3-6 Weeks)](#phase-2-multi-agent-critics-3-6-weeks)
4. [Phase 3: RAG Examples (3-4 Weeks, Optional)](#phase-3-rag-examples-3-4-weeks-optional)
5. [Technical Architecture](#technical-architecture)
6. [Implementation Details](#implementation-details)
7. [Cost-Benefit Analysis](#cost-benefit-analysis)
8. [Success Metrics](#success-metrics)
9. [Decision Points](#decision-points)
10. [Risk Mitigation](#risk-mitigation)

---

## Research Findings Summary

### Key Insights from Research

#### 1. Your Current System is Solid
- ‚úÖ Already using best practices (Constitutional AI)
- ‚úÖ Quality principles are research-backed
- ‚úÖ Architecture is maintainable
- ‚ö†Ô∏è Needs strategic layering, not replacement

#### 2. Architecture Options Evaluated

| Approach | Quality Gain | Dev Time | Cost Multiplier | Complexity | Recommendation |
|----------|--------------|----------|-----------------|------------|----------------|
| **Multi-Agent Critics** | +60-80% | 3-6 weeks | 4-8x | High | ‚úÖ **Best for quality** |
| **Ensemble Generation** | +40-60% | 1-2 weeks | 3-5x | Medium | Alternative |
| **RAG Examples** | +30-50% | 3-4 weeks | Marginal | Medium-High | ‚úÖ **Phase 3** |
| **Knowledge Graph** | +20-40% | 2-3 weeks | Marginal | Medium-High | Only if needed |
| **LoRA Fine-Tuning** | +40-70% | 4-6 weeks | Upfront $$$ | High | Not for MVP |
| **Chain-of-Thought** | +10-20% | 1-3 days | +20% | Low | ‚úÖ **Quick win** |
| **Clich√© Detection** | +10-30% | 1-2 days | Marginal | Low | ‚úÖ **Quick win** |

#### 3. Sanderson's Principles are 70% Codifiable

**Key Gaps in Current System**:
1. **Limitation Enforcement** (Sanderson's 2nd Law)
   - Current: Asks for 1-2 limitations
   - Target: 4 types √ó 1-2 each = 6-8 limitations

2. **Interconnection Tracking**
   - Current: Elements exist independently
   - Target: Each element affects 3+ domains

3. **Expansion Philosophy** (Sanderson's 3rd Law)
   - Current: No explicit guidance
   - Target: "Expand before add" principle

4. **Genre Flexibility**
   - Current: Fantasy-focused
   - Target: Universal across all genres

---

## Phase 1: Quick Wins (1 Week)

### Overview
- **Timeline**: 5-7 days
- **Quality Improvement**: +30-40%
- **Cost Increase**: +10-20% tokens (minimal)
- **Risk**: Low
- **Sanderson Score**: 6.7 ‚Üí 7.5/10

### Goals
1. Implement Chain-of-Thought reasoning
2. Add simple clich√© detection
3. Create genre selection system
4. Enforce Sanderson's limitation types
5. Clean up unused code

---

### Task 1.0: Secure API Key Input (NEW)
**Timeline**: Day 1 (4-6 hours)
**Files**: `frontend/src/components/ApiKeyInput.svelte` (new)

#### What & Why
- Users provide their own OpenAI API keys (BYOK)
- Keys stored only in memory (component state)
- Never persisted to localStorage/sessionStorage
- Direct calls from browser to OpenAI (you never see keys)
- Cost to you: $0

#### Implementation
```svelte
<!-- ApiKeyInput.svelte -->
<script>
  export let onKeySet = (key) => {};

  let apiKey = $state(''); // Memory only
  let keyStatus = $state('');

  async function validateKey(key) {
    // Test key against OpenAI API
    const response = await fetch('https://api.openai.com/v1/models', {
      headers: { 'Authorization': `Bearer ${key}` }
    });
    return response.ok;
  }

  async function handleSubmit() {
    if (await validateKey(apiKey)) {
      keyStatus = 'valid';
      onKeySet(apiKey);
    } else {
      keyStatus = 'invalid';
    }
  }
</script>

<div class="api-key-setup">
  <h3>Enter Your OpenAI API Key</h3>
  <p class="security-note">
    üîí Your key is stored only in browser memory and used
    directly to call OpenAI. It never touches our servers.
  </p>

  <input type="password" bind:value={apiKey} placeholder="sk-..." />
  <button onclick={handleSubmit}>Validate Key</button>

  {#if keyStatus === 'valid'}
    <div class="status valid">‚úÖ Key is valid!</div>
  {/if}

  {#if keyStatus === 'invalid'}
    <div class="status invalid">‚ùå Invalid key</div>
  {/if}
</div>
```

#### Security Measures
```html
<!-- Add CSP headers in index.html -->
<meta http-equiv="Content-Security-Policy" content="
  default-src 'self';
  connect-src 'self' https://api.openai.com;
  script-src 'self' 'unsafe-inline';
">
```

#### Acceptance Criteria
- [ ] API key input component created
- [ ] Key validation works (tests against OpenAI)
- [ ] Key stored in component state only (not localStorage)
- [ ] CSP headers configured
- [ ] Security notice displayed to users
- [ ] "Clear Key" button works
- [ ] Key auto-clears on tab close

#### Testing
- Enter valid key ‚Üí should validate successfully
- Enter invalid key ‚Üí should show error
- Close browser ‚Üí reopen ‚Üí key should be gone
- Check localStorage/sessionStorage ‚Üí should be empty

---

### Task 1.1: Chain-of-Thought Integration
**Timeline**: Day 2 (6-8 hours)
**Files**: `frontend/src/prompts/worldGeneration.js`

#### What & Why
- Add reasoning step before generation
- Forces structured thinking about worldbuilding choices
- +10-20% quality improvement (research-backed)

#### Implementation
```javascript
// Modify system prompt to include reasoning step
const systemPrompt = `You are an expert worldbuilder. Before generating content, think through your approach:

<reasoning>
1. Core concept: What makes this world unique?
2. Implications: How does it affect society/economy/daily life?
3. Conflicts: What tensions naturally arise?
4. Specificity: What concrete details make it real?
5. Originality: How to avoid clich√©s?
</reasoning>

Then generate following quality principles...`;
```

#### Acceptance Criteria
- [ ] Prompts include `<reasoning>` section
- [ ] Parser extracts reasoning from response
- [ ] Reasoning displayed in UI (optional, for debugging)
- [ ] Quality improved by 10%+ in test generations

#### Testing
- Generate 10 worlds with and without CoT
- Compare quality scores
- Verify reasoning is coherent

---

### Task 1.2: Simple Clich√© Detection
**Timeline**: Day 3 (6-8 hours)
**Files**: `frontend/src/lib/clicheDetector.js` (new)

#### What & Why
- Detect common naming patterns and overused words
- Auto-flag or regenerate if clich√©s detected
- +10-30% originality improvement

#### Implementation
```javascript
// Regex patterns for common clich√©s
const CLICHE_PATTERNS = {
  naming: [
    /The\s+(?:Ancient|Dark|Forbidden|Sacred|Eternal|Shadow|Crystal|Mystic)\s+\w+/gi,
    /\w+\s+of\s+(?:Darkness|Light|Shadows|Eternity|Destiny)/gi
  ],

  descriptors: [
    /\b(?:ancient|mystical|mysterious|shadowy|ethereal|arcane|forbidden)\b/gi
  ],

  tropes: [
    /chosen\s+one/gi,
    /prophecy\s+foretold/gi,
    /light\s+vs\.?\s+dark/gi,
    /dark\s+lord/gi
  ]
};

export function detectCliches(text) {
  // Count matches
  // Calculate score
  // Return { score, detected, shouldRegenerate }
}
```

#### Acceptance Criteria
- [ ] Detects 10+ clich√© patterns (naming, descriptors, tropes)
- [ ] Calculates originality score (0-10)
- [ ] Returns list of detected clich√©s with locations
- [ ] Integration point in Constitutional AI loop
- [ ] Option to auto-regenerate if score < 7.0

#### Testing
- Test on known clich√©-heavy text (should score low)
- Test on original content (should score high)
- Verify false positive rate < 10%

---

### Task 1.3: Genre Selection System
**Timeline**: Days 4-5 (12-16 hours)
**Files**:
- `frontend/src/lib/genreDetection.js` (new)
- `frontend/src/components/GenreSelector.svelte` (new)
- `frontend/src/prompts/worldGeneration.js` (modify)

#### What & Why
- Support multiple genres (fantasy, sci-fi, horror, contemporary)
- Genre-specific quality principles and clich√© lists
- +10-20% quality through targeted guidance

#### Implementation

**Part A: Genre Detection** (Day 3, 6-8 hours)
```javascript
// Quick keyword-based detection
export function quickGenreDetect(text) {
  const genreKeywords = {
    fantasy: ['magic', 'dragon', 'wizard', 'kingdom'],
    scifi: ['technology', 'space', 'alien', 'robot'],
    horror: ['fear', 'dark', 'terror', 'nightmare'],
    // ...
  };

  // Count keyword matches
  // Return genres with confidence scores
}
```

**Part B: Genre-Specific Configs** (Day 4, 6-8 hours)
```javascript
const genreConfigs = {
  fantasy: {
    principleWeights: {
      originality: 0.35,  // Higher weight
      specificity: 0.25,
      implications: 0.25,
      consistency: 0.15
    },
    clichesToAvoid: [
      'chosen one', 'ancient evil', 'light vs dark'
    ],
    focusAreas: ['magic systems', 'cultures', 'geography']
  },

  scifi: {
    principleWeights: {
      implications: 0.35,  // Tech ‚Üí society
      specificity: 0.30,
      originality: 0.20,
      consistency: 0.15
    },
    clichesToAvoid: [
      'evil AI', 'warp drive', 'laser swords'
    ],
    focusAreas: ['technology', 'future societies', 'scientific basis']
  },

  // horror, contemporary, etc.
};
```

#### Acceptance Criteria
- [ ] Detects 4+ genres (fantasy, sci-fi, horror, contemporary)
- [ ] Genre-specific clich√© lists (10+ per genre)
- [ ] Genre-specific principle weights
- [ ] UI component for genre selection
- [ ] Genre stored in session state
- [ ] Prompts adapt based on selected genre

#### Testing
- Generate worlds in each genre
- Verify genre-appropriate clich√©s are avoided
- Test genre blends (fantasy + horror)

---

### Task 1.4: Sanderson Limitation Enforcement
**Timeline**: Day 6 (6-8 hours)
**Files**: `frontend/src/prompts/worldGeneration.js`

#### What & Why
- Enforce Sanderson's 2nd Law: "Limitations > Powers"
- Require 4 types of limitations for magic/tech systems
- Biggest gap in current system (6.7 ‚Üí 7.5 Sanderson score)

#### Implementation
```javascript
// Add to magic/tech system generation prompt
const limitationPrompt = `
LIMITATIONS (Sanderson's 2nd Law - CRITICAL):
Magic/technology systems MUST have 4 types of limitations:

1. CONSTRAINTS (What it CAN'T do):
   - At least 2 specific things the system cannot accomplish
   - Example: "Cannot affect living minds directly" or "Only works on metals"

2. COSTS (What it REQUIRES):
   - Concrete resource/energy/sacrifice needed
   - Must include specific measurements
   - Example: "1 gram of copper dust per 5 minutes" or "Drains 2 years of lifespan"

3. VULNERABILITIES (What DEFEATS it):
   - 1-2 ways the system can be blocked or countered
   - Example: "Aluminium blocks all effects" or "Salt water disrupts signals"

4. SOCIAL RESTRICTIONS (How society LIMITS it):
   - Legal, cultural, or institutional controls
   - Example: "Guild licensing required" or "Forbidden by religious law"

Without all 4 types, the system will feel like "magic" that solves problems too easily.
`;
```

#### Validation
```javascript
// Add critic that validates limitations exist
function validateLimitations(magicSystem) {
  const hasConstraints = magicSystem.constraints?.length >= 2;
  const hasCosts = magicSystem.costs && includesNumbers(magicSystem.costs);
  const hasVulnerabilities = magicSystem.vulnerabilities?.length >= 1;
  const hasSocialRestrictions = magicSystem.socialRestrictions?.length >= 1;

  return {
    valid: hasConstraints && hasCosts && hasVulnerabilities && hasSocialRestrictions,
    missing: [/* list missing types */]
  };
}
```

#### Acceptance Criteria
- [ ] Prompt explicitly requires 4 limitation types
- [ ] Each type has specific examples
- [ ] Validation function checks for all 4 types
- [ ] If missing, regenerate with stronger emphasis
- [ ] Documentation updated with Sanderson's 2nd Law

#### Testing
- Generate 10 magic systems
- Verify all have 4 limitation types
- Check that limitations are specific (not vague)

---

### Task 1.5: Code Cleanup
**Timeline**: Day 7 (2-4 hours)
**Files**: Various

#### What & Why
- Remove unused `progressiveGeneration.js` (not imported anywhere)
- Document which prompts are active
- Clean up test files and old documentation
- Reduce confusion and maintenance burden

#### Tasks
1. **Verify unused files**:
   ```bash
   # Already confirmed: progressiveGeneration.js not imported
   grep -r "progressiveGeneration" frontend/src
   ```

2. **Check other prompts**:
   ```bash
   # Check if worldExpansion.js and narrativeProgression.js are used
   grep -r "worldExpansion\|narrativeProgression" frontend/src
   ```

3. **Safe removal**:
   - Move to `docs/archive/` (don't delete, may have useful code)
   - Update README with active prompt list

4. **Documentation**:
   - Create `frontend/src/prompts/README.md` listing active prompts
   - Document purpose of each prompt file

#### Acceptance Criteria
- [ ] Unused prompt files moved to archive
- [ ] No broken imports
- [ ] README documents active prompts
- [ ] Test files cleaned up
- [ ] Old implementation docs archived

---

### Phase 1 Deliverables

**By End of Week**:
- ‚úÖ Secure API key input (BYOK approach)
- ‚úÖ Chain-of-Thought prompting integrated
- ‚úÖ Clich√© detection system operational
- ‚úÖ Genre selection with 4+ genres supported
- ‚úÖ Sanderson limitation enforcement
- ‚úÖ Codebase cleaned up and documented
- ‚úÖ CSP headers configured
- ‚úÖ Production-ready security

**Quality Metrics**:
- Current: 7.0/10 average ‚Üí Target: 7.5-8.0/10
- Current: 40-60% clich√© reduction ‚Üí Target: 60-80%
- Current: 6.7 Sanderson score ‚Üí Target: 7.5/10

**Cost**:
- Development: 1 week (48 hours)
- Infrastructure: $0 (static hosting, Vercel/Netlify free tier)
- Per-generation: $0 to you (users pay OpenAI directly via their keys)
- User cost: $0.11-0.12 per world (same as current)

---

## Phase 2: Multi-Agent Critics (3-6 Weeks)

### Overview
- **Timeline**: 3-6 weeks
- **Quality Improvement**: +60-80% over Phase 1
- **Cost Increase**: 4-8x tokens (user toggleable)
- **Risk**: Medium (new architecture, external dependencies)
- **Sanderson Score**: 7.5 ‚Üí 8.5-9.0/10

### Goals
1. Build multi-agent critic system with LangGraph
2. Create 5 specialist critic agents
3. Implement orchestration workflow
4. Add "Quality Mode" user toggle
5. Display individual critic scores in UI

---

### Task 2.1: LangGraph Setup
**Timeline**: Week 1 (40 hours)
**Files**: `frontend/package.json`, `frontend/src/lib/multiAgentCritics.js` (new)

#### What & Why
- LangGraph provides structured multi-agent workflows
- State management for complex flows
- Better than pure orchestration code
- Industry standard for multi-agent systems

#### Dependencies
```bash
npm install @langchain/langgraph @langchain/core @langchain/openai
```

#### Basic Architecture
```javascript
import { StateGraph, END } from "@langchain/langgraph";

// Define state structure
const graphState = {
  worldContent: "",
  genre: "fantasy",
  critiques: {},
  overallScore: 0,
  revisedContent: "",
  metadata: {}
};

// Build workflow
const workflow = new StateGraph({ channels: graphState });
```

#### Tasks
1. **Week 1, Days 1-2**: Install and configure LangGraph
2. **Week 1, Days 3-5**: Create basic workflow (generate ‚Üí critique ‚Üí revise)
3. **Week 1, Days 3-5**: Implement state management and persistence

#### Acceptance Criteria
- [ ] LangGraph installed and configured
- [ ] Basic state machine working (3 nodes minimum)
- [ ] State persists across nodes
- [ ] Error handling for node failures
- [ ] Can visualize workflow graph (LangGraph Studio)

---

### Task 2.2: Specialist Critic Agents
**Timeline**: Weeks 2-3 (80 hours)
**Files**: `frontend/src/lib/critics/*.js` (new files)

#### What & Why
- Each critic specializes in one quality dimension
- Parallel evaluation catches more issues
- Transparent scoring (users see individual scores)

#### Critics to Build

**1. Originality Critic** (Week 2, Days 1-2)
```javascript
// frontend/src/lib/critics/originalityCritic.js

export async function originalityCritic(state) {
  const prompt = `Evaluate originality (0-10):

  Check for:
  - Genre clich√©s (chosen ones, ancient evil, etc.)
  - Generic naming ("The [Adj] [Noun]")
  - Overused descriptors (ancient, mystical, shadowy)

  Content: ${state.worldContent}
  Genre: ${state.genre}

  Output JSON:
  {
    "score": number,
    "issues": ["specific clich√© found"],
    "examples": ["quoted text"],
    "suggestions": ["how to fix"]
  }`;

  // Call LLM
  // Parse response
  // Return updated state
}
```

**2. Specificity Critic** (Week 2, Days 3-4)
```javascript
// frontend/src/lib/critics/specificityCritic.js

export async function specificityCritic(state) {
  const prompt = `Evaluate specificity (0-10):

  Count:
  - Numbers (dates, measurements, populations)
  - Named elements (people, places, resources)
  - Materials (copper, granite, silk)
  - Sensory details (smells, sounds, textures)

  Content: ${state.worldContent}

  Output JSON:
  {
    "score": number,
    "concreteDetails": number,
    "namedElements": number,
    "vaguePhrases": ["list vague phrases"],
    "suggestions": ["add specific measurements here"]
  }`;

  // Implementation
}
```

**3. Implications Critic** (Week 2, Day 5)
```javascript
// frontend/src/lib/critics/implicationsCritic.js

export async function implicationsCritic(state) {
  const prompt = `Evaluate societal implications (0-10):

  Check if major elements show effects on:
  - Economy (trade, taxation, wealth distribution)
  - Social structure (class, daily life, customs)
  - Politics (power dynamics, governance)
  - Culture (values, taboos, traditions)

  Content: ${state.worldContent}

  Score based on depth and interconnection.

  Output JSON:
  {
    "score": number,
    "coveredDomains": ["economy", "social"],
    "missingDomains": ["politics"],
    "suggestions": ["explore political implications"]
  }`;

  // Implementation
}
```

**4. Consistency Critic** (Week 3, Days 1-2)
```javascript
// frontend/src/lib/critics/consistencyCritic.js

export async function consistencyCritic(state) {
  const prompt = `Evaluate internal consistency (0-10):

  Look for:
  - Contradictions (magic costs X, later costs Y)
  - Illogical elements (desert culture with water festivals)
  - Unexplained impossibilities
  - Rule violations

  Content: ${state.worldContent}

  Output JSON:
  {
    "score": number,
    "contradictions": ["specific contradiction"],
    "illogicalElements": ["doesn't make sense because..."],
    "suggestions": ["fix by..."]
  }`;

  // Implementation
}
```

**5. Limitations Critic (Sanderson's 2nd Law)** (Week 3, Days 3-4)
```javascript
// frontend/src/lib/critics/limitationsCritic.js

export async function limitationsCritic(state) {
  const prompt = `Evaluate limitations (Sanderson's 2nd Law):

  Check if magic/tech systems have:
  1. Constraints (what it CAN'T do) - need 2+
  2. Costs (what it REQUIRES) - must be specific
  3. Vulnerabilities (what DEFEATS it) - need 1+
  4. Social restrictions (how society LIMITS it) - need 1+

  Content: ${state.worldContent}

  Output JSON:
  {
    "score": number,
    "hasConstraints": boolean,
    "hasCosts": boolean,
    "hasVulnerabilities": boolean,
    "hasSocialRestrictions": boolean,
    "missing": ["which types are missing"],
    "suggestions": ["add these limitations"]
  }`;

  // Implementation
}
```

#### Acceptance Criteria (Per Critic)
- [ ] Critic implemented as LangGraph node
- [ ] Prompt is clear and specific
- [ ] Returns structured JSON with score + details
- [ ] Handles malformed content gracefully
- [ ] Tested on 10+ examples
- [ ] Score correlates with manual quality assessment

---

### Task 2.3: Reviser Agent
**Timeline**: Week 3, Day 5 (8 hours)
**Files**: `frontend/src/lib/critics/reviser.js` (new)

#### What & Why
- Synthesizes all critic feedback
- Rewrites content to address issues
- Only runs if overall score < 8.0

#### Implementation
```javascript
export async function reviser(state) {
  // Check if revision needed
  if (state.overallScore >= 8.0) {
    return { ...state, revisedContent: state.worldContent };
  }

  // Build revision prompt from all critiques
  const revisionPrompt = `
Original worldbuilding:
${state.worldContent}

Critiques from specialists:
${Object.entries(state.critiques).map(([critic, feedback]) => `
${critic.toUpperCase()} (${feedback.score}/10):
Issues: ${feedback.issues || feedback.missing || feedback.contradictions}
Suggestions: ${feedback.suggestions}
`).join('\n')}

Overall score: ${state.overallScore}/10 (needs to be 8.0+)

Rewrite the worldbuilding to address ALL issues while:
- Keeping the core concept
- Maintaining the genre (${state.genre})
- Following all quality principles
`;

  const llm = new ChatOpenAI({ model: "gpt-4o", temperature: 0.9 });
  const response = await llm.invoke([{ role: "user", content: revisionPrompt }]);

  return { ...state, revisedContent: response.content };
}
```

#### Acceptance Criteria
- [ ] Only runs if score < 8.0
- [ ] Incorporates all critic feedback
- [ ] Maintains core concept and genre
- [ ] Returns revised content
- [ ] Handles edge cases (all critics failed, etc.)

---

### Task 2.4: Orchestration & Quality Gate
**Timeline**: Week 4 (40 hours)
**Files**: `frontend/src/lib/multiAgentCritics.js`

#### Workflow Design
```
START
  ‚Üì
Generate Initial Content
  ‚Üì
Critics (Parallel) ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚Üí Originality Critic
                        ‚îú‚îÄ‚Üí Specificity Critic
                        ‚îú‚îÄ‚Üí Implications Critic
                        ‚îú‚îÄ‚Üí Consistency Critic
                        ‚îî‚îÄ‚Üí Limitations Critic
  ‚Üì
Calculate Overall Score (weighted average)
  ‚Üì
Quality Gate: Score >= 8.0?
  ‚îú‚îÄ‚Üí YES: Return final content
  ‚îî‚îÄ‚Üí NO: Go to Reviser
           ‚Üì
       Reviser Agent
           ‚Üì
       Return revised content
           ‚Üì
          END
```

#### Implementation
```javascript
export function buildCriticWorkflow() {
  const workflow = new StateGraph({ channels: graphState });

  // Add all critic nodes
  workflow.addNode("originalityCritic", originalityCritic);
  workflow.addNode("specificityCritic", specificityCritic);
  workflow.addNode("implicationsCritic", implicationsCritic);
  workflow.addNode("consistencyCritic", consistencyCritic);
  workflow.addNode("limitationsCritic", limitationsCritic);
  workflow.addNode("calculateScore", calculateOverallScore);
  workflow.addNode("reviser", reviser);

  // Define execution order
  // Critics run in parallel after initial generation
  workflow.addEdge("START", "originalityCritic");
  workflow.addEdge("START", "specificityCritic");
  workflow.addEdge("START", "implicationsCritic");
  workflow.addEdge("START", "consistencyCritic");
  workflow.addEdge("START", "limitationsCritic");

  // All critics converge to score calculation
  workflow.addEdge("originalityCritic", "calculateScore");
  workflow.addEdge("specificityCritic", "calculateScore");
  workflow.addEdge("implicationsCritic", "calculateScore");
  workflow.addEdge("consistencyCritic", "calculateScore");
  workflow.addEdge("limitationsCritic", "calculateScore");

  // Score calculation to reviser
  workflow.addEdge("calculateScore", "reviser");
  workflow.addEdge("reviser", END);

  return workflow.compile();
}
```

#### Weighted Scoring
```javascript
function calculateOverallScore(state) {
  const weights = {
    originality: 0.30,
    specificity: 0.25,
    implications: 0.25,
    consistency: 0.15,
    limitations: 0.05  // Already checked by implications
  };

  const score = Object.entries(state.critiques).reduce((total, [key, critique]) => {
    return total + (critique.score * weights[key]);
  }, 0);

  return { ...state, overallScore: score };
}
```

#### Acceptance Criteria
- [ ] Workflow executes all critics in parallel
- [ ] Score calculation uses weighted average
- [ ] Quality gate at 8.0 threshold
- [ ] Reviser only runs if needed
- [ ] Error handling for failed critics (use fallback score)
- [ ] Maximum 1 revision attempt (prevent infinite loops)

---

### Task 2.5: User Interface Integration
**Timeline**: Week 5 (40 hours)
**Files**:
- `frontend/src/components/WorldbuildingStudio.svelte`
- `frontend/src/components/QualityModeToggle.svelte` (new)
- `frontend/src/components/CriticScores.svelte` (new)

#### Features

**1. Quality Mode Toggle** (Days 1-2)
```svelte
<!-- QualityModeToggle.svelte -->
<script>
  export let mode = $state('fast'); // 'fast' or 'quality'
  export let onModeChange = () => {};
</script>

<div class="mode-toggle">
  <button
    class:active={mode === 'fast'}
    onclick={() => onModeChange('fast')}
  >
    ‚ö° Fast Mode
    <span class="subtitle">Current system (0.5-1 min)</span>
  </button>

  <button
    class:active={mode === 'quality'}
    onclick={() => onModeChange('quality')}
  >
    ‚ú® Quality Mode
    <span class="subtitle">Multi-agent critics (2-4 min, 4-8x cost)</span>
  </button>
</div>

<style>
  .mode-toggle {
    display: flex;
    gap: 1rem;
    margin-bottom: 2rem;
  }

  button {
    flex: 1;
    padding: 1rem;
    border: 2px solid var(--border);
    border-radius: 8px;
    background: white;
    cursor: pointer;
  }

  button.active {
    border-color: var(--primary);
    background: var(--primary-light);
  }

  .subtitle {
    display: block;
    font-size: 0.85rem;
    color: var(--text-secondary);
    margin-top: 0.25rem;
  }
</style>
```

**2. Critic Scores Display** (Days 3-4)
```svelte
<!-- CriticScores.svelte -->
<script>
  export let critiques = {};
  export let overallScore = 0;
</script>

{#if Object.keys(critiques).length > 0}
  <div class="critic-scores">
    <h3>Quality Analysis</h3>

    <div class="overall-score" class:excellent={overallScore >= 8.5} class:good={overallScore >= 8.0}>
      <span class="label">Overall Score</span>
      <span class="score">{overallScore.toFixed(1)}/10</span>
    </div>

    <div class="individual-scores">
      {#each Object.entries(critiques) as [critic, feedback]}
        <div class="critic-card">
          <div class="critic-header">
            <h4>{critic}</h4>
            <span class="score" class:warning={feedback.score < 7}>
              {feedback.score}/10
            </span>
          </div>

          {#if feedback.issues?.length > 0}
            <details>
              <summary>Issues Found ({feedback.issues.length})</summary>
              <ul>
                {#each feedback.issues as issue}
                  <li>{issue}</li>
                {/each}
              </ul>
            </details>
          {/if}

          {#if feedback.suggestions?.length > 0}
            <details>
              <summary>Suggestions ({feedback.suggestions.length})</summary>
              <ul>
                {#each feedback.suggestions as suggestion}
                  <li>{suggestion}</li>
                {/each}
              </ul>
            </details>
          {/if}
        </div>
      {/each}
    </div>
  </div>
{/if}
```

**3. Integration with Main Component** (Day 5)
```javascript
// In WorldbuildingStudio.svelte

import { generateWithMultiAgentCritics } from '../lib/multiAgentCritics.js';
import QualityModeToggle from './QualityModeToggle.svelte';
import CriticScores from './CriticScores.svelte';

let qualityMode = $state('fast');
let critiques = $state({});
let overallScore = $state(0);

async function generateWorld() {
  setLoading(true);

  try {
    // Initial generation
    const initialContent = await generateInitialWorld(userInput);

    if (qualityMode === 'quality') {
      // Use multi-agent critics
      const result = await generateWithMultiAgentCritics(initialContent, selectedGenre);

      worldContent = result.finalContent;
      critiques = result.critiques;
      overallScore = result.overallScore;

      showNotification(`Quality Mode: ${result.revised ? 'Content revised' : 'No revision needed'}`);
    } else {
      // Fast mode: use existing Constitutional AI
      worldContent = initialContent;
    }
  } catch (error) {
    handleError(error);
  } finally {
    setLoading(false);
  }
}
```

#### Acceptance Criteria
- [ ] Quality mode toggle works (switches between fast/quality)
- [ ] Mode preference saved in session/localStorage
- [ ] Loading states for quality mode (longer wait time)
- [ ] Critic scores displayed clearly
- [ ] Can expand individual critic feedback
- [ ] Overall score has visual indicators (excellent/good/needs work)
- [ ] Mobile-responsive design

---

### Task 2.6: Testing & Optimization
**Timeline**: Week 6 (40 hours)
**Files**: `frontend/tests/multi-agent-quality.spec.js` (new)

#### Test Categories

**1. Unit Tests** (Days 1-2)
- Each critic scores correctly
- Score calculation is accurate
- Reviser improves content
- Error handling works

**2. Integration Tests** (Days 3-4)
- Full workflow executes successfully
- State persists across nodes
- Parallel execution works
- Quality gate triggers correctly

**3. Quality Tests** (Day 5)
- Generate 50 worlds in quality mode
- Compare against fast mode
- Measure quality improvement
- Verify 80%+ pass quality gate without revision

**4. Performance Optimization** (Day 5)
- Reduce latency where possible
- Optimize parallel execution
- Cache common critic prompts
- Monitor token usage

#### Success Criteria
- [ ] 95%+ workflow success rate
- [ ] Average quality score 8.5+/10
- [ ] 80%+ worlds pass without revision
- [ ] <5 minute generation time
- [ ] Token usage within 4-8x estimate

---

### Phase 2 Deliverables

**By End of 6 Weeks**:
- ‚úÖ LangGraph multi-agent system operational
- ‚úÖ 5 specialist critics implemented
- ‚úÖ Quality mode toggle in UI
- ‚úÖ Critic scores displayed to users
- ‚úÖ Comprehensive test suite

**Quality Metrics**:
- Phase 1: 7.5/10 ‚Üí Target: 8.5-9.0/10
- Phase 1: 60-80% clich√© reduction ‚Üí Target: 85-95%
- Phase 1: 7.5 Sanderson score ‚Üí Target: 8.5-9.0/10

**Cost**:
- Development: 3-6 weeks (120-240 hours)
- Per-generation (Quality Mode): $0.40-0.80 (4-8x)
- Per-generation (Fast Mode): $0.10-0.12 (unchanged)

---

## Phase 3: RAG Examples (3-4 Weeks, Optional)

### Overview
- **Timeline**: 3-4 weeks
- **Quality Improvement**: +30-50% over Phase 2
- **Cost Increase**: Marginal (+$0.05/generation)
- **Risk**: Medium (requires curation, copyright concerns)
- **Sanderson Score**: 8.5 ‚Üí 9.0-9.5/10 (master-crafted ceiling)

### Decision Point
**Evaluate before starting Phase 3**:
- Is Phase 2 quality (8.5/10) sufficient for MVP?
- Do we have budget for example curation?
- Are copyright concerns manageable?

---

### Task 3.1: Example Curation
**Timeline**: Weeks 1-2 (60-80 hours)
**Files**: `docs/examples/` directory structure

#### What & Why
- Collect 200-500 high-quality worldbuilding excerpts
- Annotate with quality dimensions
- Organize by genre and principle
- Most time-consuming task (60% of Phase 3 effort)

#### Example Sources
1. **Published Works** (Fair Use / Public Domain)
   - Tolkien (public domain in some jurisdictions)
   - Sanderson (short excerpts for educational use)
   - Classic sci-fi (Asimov, Clarke - public domain)
   - Classic horror (Lovecraft - public domain)

2. **Worldbuilding Guides**
   - Sanderson's Laws (public, educational)
   - Published RPG settings (with permission)
   - Worldbuilding Stack Exchange (Creative Commons)

3. **Created Examples**
   - Hand-write 50-100 examples
   - Commission examples from writers
   - Use AI-generated + human-curated examples

#### Structure
```
docs/examples/
‚îú‚îÄ‚îÄ fantasy/
‚îÇ   ‚îú‚îÄ‚îÄ originality/
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ 001-unique-magic-system.md
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ 002-fresh-culture.md
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ ...
‚îÇ   ‚îú‚îÄ‚îÄ specificity/
‚îÇ   ‚îú‚îÄ‚îÄ implications/
‚îÇ   ‚îî‚îÄ‚îÄ consistency/
‚îú‚îÄ‚îÄ scifi/
‚îÇ   ‚îú‚îÄ‚îÄ originality/
‚îÇ   ‚îú‚îÄ‚îÄ specificity/
‚îÇ   ‚îî‚îÄ‚îÄ ...
‚îú‚îÄ‚îÄ horror/
‚îî‚îÄ‚îÄ contemporary/
```

#### Example Format
```markdown
---
id: fantasy-originality-001
genre: fantasy
principle: originality
source: "Brandon Sanderson, Mistborn: The Final Empire"
copyright: "Fair use excerpt for educational purposes"
quality_score: 9.5/10
---

# Unique Magic System: Allomancy

Allomancy is the hereditary ability to ingest and "burn" metals, granting specific powers. Each of 16 metals provides a different ability. For example:

- **Iron**: Pulls on nearby sources of metal
- **Steel**: Pushes on nearby sources of metal
- **Tin**: Enhances senses
- **Pewter**: Enhances physical strength

**Why This is Excellent:**
- Specific limitations (only 16 metals, each with one power)
- Clear costs (must ingest and burn metal)
- Internal consistency (push/pull physics obey Newton's laws)
- Societal implications (creates class system: nobles have powers, skaa don't)
- Original concept (not traditional spell-casting)

**Key Techniques:**
- Hard magic system with clear rules
- Metal as resource creates economy
- Genetic inheritance creates social structure
- Physics-based (Pushing/Pulling) feels grounded
```

#### Tasks
1. **Week 1**: Curate 100 examples (5-10 per day)
2. **Week 2**: Curate 100 more examples
3. **Throughout**: Annotate with quality dimensions

#### Acceptance Criteria
- [ ] 200+ examples collected (target: 300-500)
- [ ] Coverage across 4 genres (fantasy, sci-fi, horror, contemporary)
- [ ] Coverage across 5 principles (originality, specificity, implications, consistency, limitations)
- [ ] All examples annotated with quality scores
- [ ] Copyright/fair use documented
- [ ] Markdown format for easy parsing

---

### Task 3.2: Vector Database Setup
**Timeline**: Week 3, Days 1-3 (24 hours)
**Files**: `frontend/src/lib/exampleDatabase.js` (new)

#### What & Why
- Store examples with embeddings for semantic search
- Retrieve most relevant examples based on user request
- Fast lookup (<100ms)

#### Technology Choice
**Option A: Supabase pgvector** (Recommended)
- Pros: Free tier, PostgreSQL-based, easy setup
- Cons: Requires Supabase account

**Option B: Pinecone**
- Pros: Specialized for vector search, very fast
- Cons: $20+/month

**Option C: In-Memory (Chroma.js)**
- Pros: No external dependency, free
- Cons: Reloads embeddings on startup (slow)

**Recommendation**: Start with Option A (Supabase), migrate to C if needed.

#### Implementation
```javascript
// frontend/src/lib/exampleDatabase.js

import { createClient } from '@supabase/supabase-js';
import { OpenAIEmbeddings } from '@langchain/openai';

const supabase = createClient(SUPABASE_URL, SUPABASE_KEY);
const embeddings = new OpenAIEmbeddings({ model: "text-embedding-3-small" });

// Embed and store all examples (one-time setup)
export async function buildExampleDatabase(examples) {
  for (const example of examples) {
    const embedding = await embeddings.embedQuery(example.content);

    await supabase.from('worldbuilding_examples').insert({
      id: example.id,
      genre: example.genre,
      principle: example.principle,
      content: example.content,
      explanation: example.explanation,
      quality_score: example.quality_score,
      embedding: embedding
    });
  }
}

// Retrieve relevant examples
export async function retrieveExamples(userRequest, genre, principle, topK = 3) {
  const queryEmbedding = await embeddings.embedQuery(userRequest);

  // Supabase vector similarity search
  const { data, error } = await supabase.rpc('match_examples', {
    query_embedding: queryEmbedding,
    match_threshold: 0.7,  // Minimum similarity
    match_count: topK,
    filter_genre: genre,
    filter_principle: principle
  });

  return data;
}
```

#### Database Schema
```sql
-- Supabase table schema
CREATE TABLE worldbuilding_examples (
  id TEXT PRIMARY KEY,
  genre TEXT NOT NULL,
  principle TEXT NOT NULL,
  content TEXT NOT NULL,
  explanation TEXT,
  quality_score FLOAT,
  embedding VECTOR(1536),  -- OpenAI embedding dimension
  created_at TIMESTAMPTZ DEFAULT NOW()
);

-- Vector similarity search function
CREATE OR REPLACE FUNCTION match_examples(
  query_embedding VECTOR(1536),
  match_threshold FLOAT,
  match_count INT,
  filter_genre TEXT DEFAULT NULL,
  filter_principle TEXT DEFAULT NULL
)
RETURNS TABLE (
  id TEXT,
  genre TEXT,
  principle TEXT,
  content TEXT,
  explanation TEXT,
  quality_score FLOAT,
  similarity FLOAT
)
LANGUAGE plpgsql
AS $$
BEGIN
  RETURN QUERY
  SELECT
    e.id,
    e.genre,
    e.principle,
    e.content,
    e.explanation,
    e.quality_score,
    1 - (e.embedding <=> query_embedding) AS similarity
  FROM worldbuilding_examples e
  WHERE (filter_genre IS NULL OR e.genre = filter_genre)
    AND (filter_principle IS NULL OR e.principle = filter_principle)
    AND 1 - (e.embedding <=> query_embedding) > match_threshold
  ORDER BY e.embedding <=> query_embedding
  LIMIT match_count;
END;
$$;
```

#### Tasks
1. **Day 1**: Set up Supabase account and database
2. **Day 2**: Create schema and vector search function
3. **Day 3**: Embed all examples and insert into database

#### Acceptance Criteria
- [ ] Database set up with vector support
- [ ] All examples embedded and stored
- [ ] Retrieval function works (<100ms)
- [ ] Returns relevant examples (manual verification)
- [ ] Filters by genre and principle work

---

### Task 3.3: Integration with Generation
**Timeline**: Week 3, Days 4-5 (16 hours)
**Files**: `frontend/src/lib/multiAgentCritics.js` (modify)

#### What & Why
- Include retrieved examples in generation prompts
- Show AI concrete examples of quality
- "Here's how Tolkien handles specificity..."

#### Implementation

**Modify Generation Prompt**:
```javascript
async function generateWithExamples(userConcept, genre) {
  // Retrieve relevant examples (2-3 per principle)
  const [originalityExamples, specificityExamples, implicationsExamples] = await Promise.all([
    retrieveExamples(userConcept, genre, 'originality', 2),
    retrieveExamples(userConcept, genre, 'specificity', 2),
    retrieveExamples(userConcept, genre, 'implications', 2)
  ]);

  const examplesContext = `
Here are examples of master-crafted worldbuilding:

## ORIGINALITY (from ${originalityExamples[0].source})
${originalityExamples[0].content}

Why this is excellent: ${originalityExamples[0].explanation}

## SPECIFICITY (from ${specificityExamples[0].source})
${specificityExamples[0].content}

Why this is excellent: ${specificityExamples[0].explanation}

## IMPLICATIONS (from ${implicationsExamples[0].source})
${implicationsExamples[0].content}

Why this is excellent: ${implicationsExamples[0].explanation}

---

Now create worldbuilding with this level of quality for: ${userConcept}
`;

  return await generateWorld(examplesContext + basePrompt);
}
```

**Modify Critic Prompts**:
```javascript
// Each critic can also reference examples
async function originalityCritic(state) {
  const examples = await retrieveExamples(state.worldContent, state.genre, 'originality', 1);

  const prompt = `Compare this worldbuilding against a master example:

MASTER EXAMPLE (${examples[0].quality_score}/10):
${examples[0].content}

Why it's excellent: ${examples[0].explanation}

WORLDBUILDING TO EVALUATE:
${state.worldContent}

Score 0-10. How does it compare to the master example?`;

  // ...
}
```

#### Acceptance Criteria
- [ ] Examples retrieved before generation
- [ ] Examples included in generation prompt
- [ ] Examples included in critic prompts
- [ ] Relevant examples (not random)
- [ ] Quality improves measurably (+20% minimum)

---

### Task 3.4: Testing & Evaluation
**Timeline**: Week 4 (40 hours)
**Files**: `frontend/tests/rag-quality-evaluation.spec.js` (new)

#### Test Categories

**1. Retrieval Quality** (Days 1-2)
- Test 50 queries
- Verify retrieved examples are relevant
- Measure similarity scores
- Check genre/principle filtering

**2. Generation Quality** (Days 3-4)
- Generate 50 worlds with RAG
- Generate 50 worlds without RAG
- Compare quality scores
- Verify +20% improvement minimum

**3. A/B Testing** (Day 5)
- User study with 10 participants
- Half see RAG-generated worlds, half don't
- Collect quality ratings
- Analyze statistical significance

#### Success Criteria
- [ ] 90%+ retrieval relevance
- [ ] +30-50% quality improvement vs. Phase 2
- [ ] User preference for RAG-generated content
- [ ] Cost remains under +$0.10/generation

---

### Phase 3 Deliverables

**By End of 4 Weeks**:
- ‚úÖ 200-500 curated examples
- ‚úÖ Vector database operational
- ‚úÖ RAG integrated with generation
- ‚úÖ Quality evaluation complete

**Quality Metrics**:
- Phase 2: 8.5/10 ‚Üí Target: 9.0-9.5/10
- Phase 2: 85-95% clich√© reduction ‚Üí Target: 90-98%
- Phase 2: 8.5 Sanderson score ‚Üí Target: 9.0-9.5/10
- **Approaching master-crafted quality**

**Cost**:
- Development: 3-4 weeks (100-140 hours)
- Infrastructure: $20-50/month (Supabase/Pinecone)
- Per-generation: +$0.02-0.05 (embeddings)

---

## Technical Architecture

### System Diagram

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ                         USER                                 ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                         ‚îÇ
                         ‚ñº
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ                  FRONTEND (Svelte)                           ‚îÇ
‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê   ‚îÇ
‚îÇ  ‚îÇ  WorldbuildingStudio.svelte                          ‚îÇ   ‚îÇ
‚îÇ  ‚îÇ  - User input                                        ‚îÇ   ‚îÇ
‚îÇ  ‚îÇ  - Genre selection                                   ‚îÇ   ‚îÇ
‚îÇ  ‚îÇ  - Quality mode toggle (Fast / Quality)             ‚îÇ   ‚îÇ
‚îÇ  ‚îÇ  - Display results + critic scores                  ‚îÇ   ‚îÇ
‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò   ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                         ‚îÇ
                         ‚ñº
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ              GENERATION PIPELINE                             ‚îÇ
‚îÇ                                                              ‚îÇ
‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê         ‚îÇ
‚îÇ  ‚îÇ  FAST MODE (Current System)                    ‚îÇ         ‚îÇ
‚îÇ  ‚îÇ  - Prompt Engineering                          ‚îÇ         ‚îÇ
‚îÇ  ‚îÇ  - Constitutional AI (1 critique loop)         ‚îÇ         ‚îÇ
‚îÇ  ‚îÇ  - Chain-of-Thought                            ‚îÇ         ‚îÇ
‚îÇ  ‚îÇ  - Clich√© Detection                            ‚îÇ         ‚îÇ
‚îÇ  ‚îÇ  - Genre-Specific Config                       ‚îÇ         ‚îÇ
‚îÇ  ‚îÇ  Time: 30-60s | Cost: $0.10-0.12              ‚îÇ         ‚îÇ
‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò         ‚îÇ
‚îÇ                                                              ‚îÇ
‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê         ‚îÇ
‚îÇ  ‚îÇ  QUALITY MODE (Multi-Agent)                    ‚îÇ         ‚îÇ
‚îÇ  ‚îÇ  - Initial Generation                          ‚îÇ         ‚îÇ
‚îÇ  ‚îÇ  - LangGraph Orchestration                     ‚îÇ         ‚îÇ
‚îÇ  ‚îÇ  - 5 Specialist Critics (parallel)             ‚îÇ         ‚îÇ
‚îÇ  ‚îÇ  - Weighted Scoring                            ‚îÇ         ‚îÇ
‚îÇ  ‚îÇ  - Quality Gate (8.0 threshold)                ‚îÇ         ‚îÇ
‚îÇ  ‚îÇ  - Reviser Agent (if needed)                   ‚îÇ         ‚îÇ
‚îÇ  ‚îÇ  Time: 2-4min | Cost: $0.40-0.80              ‚îÇ         ‚îÇ
‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò         ‚îÇ
‚îÇ                                                              ‚îÇ
‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê         ‚îÇ
‚îÇ  ‚îÇ  RAG LAYER (Optional, Phase 3)                 ‚îÇ         ‚îÇ
‚îÇ  ‚îÇ  - Example Retrieval (vector search)           ‚îÇ         ‚îÇ
‚îÇ  ‚îÇ  - Context Enhancement                         ‚îÇ         ‚îÇ
‚îÇ  ‚îÇ  - Included in generation prompts              ‚îÇ         ‚îÇ
‚îÇ  ‚îÇ  - Included in critic prompts                  ‚îÇ         ‚îÇ
‚îÇ  ‚îÇ  Additional: +$0.02-0.05                       ‚îÇ         ‚îÇ
‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò         ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                         ‚îÇ
                         ‚ñº
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ                    DATA LAYER                                ‚îÇ
‚îÇ  - Session State (genre, mode, history)                     ‚îÇ
‚îÇ  - Vector Database (examples) [Phase 3]                     ‚îÇ
‚îÇ  - Knowledge Graph (consistency) [Future]                   ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

### File Structure

```
frontend/
‚îú‚îÄ‚îÄ src/
‚îÇ   ‚îú‚îÄ‚îÄ components/
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ WorldbuildingStudio.svelte (main interface)
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ QualityModeToggle.svelte (fast/quality toggle)
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ CriticScores.svelte (display critic feedback)
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ GenreSelector.svelte (genre selection)
‚îÇ   ‚îÇ
‚îÇ   ‚îú‚îÄ‚îÄ lib/
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ openai.js (API calls)
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ constitutionalAI.js (Phase 1 system)
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ clicheDetector.js (Phase 1 - NEW)
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ genreDetection.js (Phase 1 - NEW)
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ multiAgentCritics.js (Phase 2 - NEW)
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ exampleDatabase.js (Phase 3 - NEW)
‚îÇ   ‚îÇ   ‚îÇ
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ critics/ (Phase 2 - NEW)
‚îÇ   ‚îÇ       ‚îú‚îÄ‚îÄ originalityCritic.js
‚îÇ   ‚îÇ       ‚îú‚îÄ‚îÄ specificityCritic.js
‚îÇ   ‚îÇ       ‚îú‚îÄ‚îÄ implicationsCritic.js
‚îÇ   ‚îÇ       ‚îú‚îÄ‚îÄ consistencyCritic.js
‚îÇ   ‚îÇ       ‚îú‚îÄ‚îÄ limitationsCritic.js
‚îÇ   ‚îÇ       ‚îî‚îÄ‚îÄ reviser.js
‚îÇ   ‚îÇ
‚îÇ   ‚îî‚îÄ‚îÄ prompts/
‚îÇ       ‚îú‚îÄ‚îÄ worldGeneration.js (MODIFY for Phase 1)
‚îÇ       ‚îú‚îÄ‚îÄ worldExpansion.js (keep)
‚îÇ       ‚îî‚îÄ‚îÄ narrativeProgression.js (check if used)
‚îÇ
‚îú‚îÄ‚îÄ tests/
‚îÇ   ‚îú‚îÄ‚îÄ cliche-detection.spec.js (Phase 1)
‚îÇ   ‚îú‚îÄ‚îÄ genre-detection.spec.js (Phase 1)
‚îÇ   ‚îú‚îÄ‚îÄ multi-agent-quality.spec.js (Phase 2)
‚îÇ   ‚îî‚îÄ‚îÄ rag-quality-evaluation.spec.js (Phase 3)
‚îÇ
‚îî‚îÄ‚îÄ package.json (add LangGraph dependencies)

docs/
‚îú‚îÄ‚îÄ MASTER_QUALITY_IMPLEMENTATION_PLAN.md (this document)
‚îú‚îÄ‚îÄ SANDERSON_WORLDBUILDING_RESEARCH.md (research findings)
‚îú‚îÄ‚îÄ examples/ (Phase 3)
‚îÇ   ‚îú‚îÄ‚îÄ fantasy/
‚îÇ   ‚îú‚îÄ‚îÄ scifi/
‚îÇ   ‚îú‚îÄ‚îÄ horror/
‚îÇ   ‚îî‚îÄ‚îÄ contemporary/
‚îî‚îÄ‚îÄ archive/ (old/unused code)
```

---

## Cost-Benefit Analysis

### Development Costs

| Phase | Timeline | Effort (hours) | Equivalent Cost @ $50/hr |
|-------|----------|----------------|--------------------------|
| **Phase 1: Quick Wins** | 1 week | 40 hours | $2,000 |
| **Phase 2: Multi-Agent** | 3-6 weeks | 120-240 hours | $6,000-12,000 |
| **Phase 3: RAG Examples** | 3-4 weeks | 100-140 hours | $5,000-7,000 |
| **Total** | 7-11 weeks | 260-420 hours | $13,000-21,000 |

### Per-Generation Costs

| Mode | Token Usage | Cost @ GPT-4o ($2.50/1M in, $10/1M out) | Time |
|------|-------------|----------------------------------------|------|
| **Fast Mode (Current)** | ~4,000 tokens | $0.10 | 30-60s |
| **Fast Mode (Phase 1)** | ~4,500 tokens | $0.11-0.12 | 30-60s |
| **Quality Mode (Phase 2)** | ~16,000-32,000 tokens | $0.40-0.80 | 2-4min |
| **Quality + RAG (Phase 3)** | +800 tokens | +$0.02-0.05 | 2-4min |

### ROI Calculation

**Scenario**: 10,000 worlds generated per month

| Phase | Dev Cost | Monthly Gen Cost | Quality Improvement | ROI |
|-------|----------|------------------|---------------------|-----|
| **Baseline** | $0 | $1,000 (all fast) | 0% | - |
| **Phase 1** | $2,000 | $1,100 (all fast) | +30-40% | ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê Excellent |
| **Phase 2** | $8,000 | $2,500 (50/50 split) | +60-80% | ‚≠ê‚≠ê‚≠ê‚≠ê Very Good |
| **Phase 3** | $13,000 | $3,000 (50/50 + RAG) | +100%+ | ‚≠ê‚≠ê‚≠ê Good |

**Break-Even Analysis**:
- Phase 1: <1 month (if users value quality)
- Phase 2: 3-6 months (if 50%+ use Quality Mode)
- Phase 3: 6-12 months (if quality justifies premium pricing)

---

## Success Metrics

### Quality Metrics

**Phase 1 Targets**:
- [ ] Average quality score: 7.5-8.0/10 (vs. 7.0 baseline)
- [ ] Clich√© reduction: 60-80% (vs. 40-60% baseline)
- [ ] Sanderson alignment: 7.5/10 (vs. 6.7 baseline)
- [ ] User satisfaction: 80%+ rate as "good" or better

**Phase 2 Targets**:
- [ ] Average quality score: 8.5-9.0/10 (Quality Mode)
- [ ] Clich√© reduction: 85-95%
- [ ] Sanderson alignment: 8.5-9.0/10
- [ ] Quality gate pass rate: 80%+ (no revision needed)
- [ ] User satisfaction: 90%+ rate as "excellent"

**Phase 3 Targets**:
- [ ] Average quality score: 9.0-9.5/10
- [ ] Clich√© reduction: 90-98%
- [ ] Sanderson alignment: 9.0-9.5/10
- [ ] Master-crafted ceiling reached

### Performance Metrics

**Phase 1**:
- [ ] Generation time: 30-60s (no increase)
- [ ] API success rate: 95%+
- [ ] Error rate: <5%

**Phase 2**:
- [ ] Generation time (Quality Mode): <5min
- [ ] Critic success rate: 95%+ per critic
- [ ] Workflow completion rate: 95%+

**Phase 3**:
- [ ] Retrieval time: <100ms
- [ ] Retrieval relevance: 90%+
- [ ] No significant latency increase

### User Experience Metrics

- [ ] Time to first world: <2min (Fast Mode) or <5min (Quality Mode)
- [ ] User retention: 70%+ return for second world
- [ ] Quality rating: 4.5+ stars average
- [ ] Feature awareness: 80%+ know about Quality Mode
- [ ] Feature usage: 30%+ use Quality Mode at least once

### Business Metrics

- [ ] User acquisition: Track new users
- [ ] Conversion rate: Free ‚Üí Paid (if applicable)
- [ ] Churn rate: <10% monthly
- [ ] Net Promoter Score: 50+
- [ ] Cost per world: Sustainable unit economics

---

## Decision Points

### Decision Point 1: After Phase 1 (Week 1)
**Question**: Is +30-40% quality improvement enough, or continue to Phase 2?

**Evaluate**:
- User feedback on Phase 1 quality
- Quality scores hitting 7.5-8.0/10 target?
- Budget available for Phase 2?
- Timeline constraints?

**Options**:
- ‚úÖ **Continue to Phase 2**: If quality still below expectations
- ‚úÖ **Ship Phase 1 as MVP**: If quality is sufficient, gather user feedback
- ‚úÖ **Iterate on Phase 1**: If specific issues can be fixed quickly

**Go/No-Go Criteria**:
- GO if: Quality score < 7.5/10 OR user feedback demands higher quality
- NO-GO if: Quality score > 8.0/10 AND users satisfied

---

### Decision Point 2: After Phase 2 (Week 7)
**Question**: Is 8.5/10 quality "good enough," or pursue Phase 3 for master-crafted ceiling?

**Evaluate**:
- Are we hitting 8.5-9.0/10 consistently?
- Do users perceive quality as "excellent"?
- Is there demand for even higher quality?
- Budget for Phase 3 curation?

**Options**:
- ‚úÖ **Continue to Phase 3**: If targeting professional writers, premium tier
- ‚úÖ **Launch without Phase 3**: If 8.5/10 meets market needs
- ‚úÖ **Partial Phase 3**: Build database with 50-100 examples (not 200-500)

**Go/No-Go Criteria**:
- GO if: Targeting premium market OR user demand for perfection
- NO-GO if: Quality is sufficient for 90%+ use cases

---

### Decision Point 3: Architecture Alternatives
**Question**: Multi-agent critics (Phase 2) or ensemble generation?

**If Multi-Agent Critics Fail or Too Complex**:
- Alternative: Implement ensemble generation instead
  - Generate 3-5 worlds in parallel
  - Score each with current Constitutional AI
  - Select best one
  - Simpler architecture, similar quality gain

**Fallback Plan**:
- Week 2 of Phase 2: If LangGraph integration is problematic
- Pivot to ensemble approach (1 week to implement)
- Lower complexity, similar quality improvement

---

## Risk Mitigation

### Technical Risks

**Risk 1: LangGraph Complexity**
- **Probability**: Medium
- **Impact**: High (could block Phase 2)
- **Mitigation**:
  - Prototype LangGraph in Week 1 before full commitment
  - Have ensemble generation as backup plan
  - Consider simpler orchestration (plain async/await)

**Risk 2: Critic Quality**
- **Probability**: Medium
- **Impact**: Medium (critics might not improve quality)
- **Mitigation**:
  - Test each critic individually before integration
  - Iterate on prompts until 90%+ accuracy
  - Use human evaluation to validate critic scoring

**Risk 3: API Rate Limits / Costs**
- **Probability**: Low-Medium
- **Impact**: High (could make Quality Mode unusable)
- **Mitigation**:
  - Implement rate limiting and queuing
  - Batch requests where possible
  - Cache critic results for similar content
  - Monitor costs closely, set alerts

**Risk 4: RAG Retrieval Quality**
- **Probability**: Medium
- **Impact**: Medium (Phase 3 might not improve quality)
- **Mitigation**:
  - Test retrieval relevance early (before full integration)
  - Tune similarity thresholds carefully
  - A/B test with and without RAG before full rollout

### Project Risks

**Risk 5: Timeline Slippage**
- **Probability**: High (typical for multi-phase projects)
- **Impact**: Medium (delayed launch)
- **Mitigation**:
  - Build in 20% buffer for each phase
  - Ship Phase 1 as MVP if schedule pressures
  - Parallelize where possible (e.g., curation during dev)

**Risk 6: Scope Creep**
- **Probability**: High
- **Impact**: Medium-High (never ship)
- **Mitigation**:
  - Strict phase boundaries
  - Ship Phase 1, get user feedback before Phase 2
  - Resist adding new features mid-phase

**Risk 7: Copyright Issues (Phase 3)**
- **Probability**: Low-Medium
- **Impact**: High (legal concerns)
- **Mitigation**:
  - Prioritize public domain and Creative Commons content
  - Use short excerpts (fair use doctrine)
  - Consult legal counsel if using copyrighted excerpts
  - Option: Commission original examples

### Quality Risks

**Risk 8: Quality Not Improving as Expected**
- **Probability**: Medium
- **Impact**: High (defeats purpose of project)
- **Mitigation**:
  - Test quality improvement at each phase
  - Use multiple evaluation methods (automated + human)
  - Iterate on prompts/critics until improvement verified
  - Have quantitative targets, not just subjective assessment

**Risk 9: Increased Costs Reduce Adoption**
- **Probability**: Medium (if Quality Mode is 8x cost)
- **Impact**: High (feature not used)
- **Mitigation**:
  - Keep Fast Mode as default, free tier
  - Position Quality Mode as premium option
  - Optimize to reduce costs (caching, batching)
  - Consider subscription model to amortize costs

---

## Next Steps

### Immediate (This Week)
1. **Review this plan** - Discuss with team, get alignment
2. **Finalize Phase 1 scope** - Any additions/subtractions?
3. **Set up project tracking** - GitHub issues, Kanban board
4. **Prepare development environment** - Dependencies, testing infrastructure

### Week 1 (Phase 1 Start)
1. **Day 1**: Implement Chain-of-Thought
2. **Day 2**: Build clich√© detector
3. **Days 3-4**: Genre selection system
4. **Day 5**: Sanderson limitation enforcement
5. **Day 6**: Code cleanup

### Decision Points Schedule
- **End of Week 1**: Evaluate Phase 1 results, decide on Phase 2
- **End of Week 7**: Evaluate Phase 2 results, decide on Phase 3
- **End of Week 11**: Ship final product or iterate

---

## Appendix A: Research Sources

### Academic Papers
1. Constitutional AI: Bai et al. (2022) - arXiv:2212.08073
2. Weaver (Fine-tuned Writing Model): arXiv:2401.17268
3. LLM-Blender (Ensemble): arXiv:2306.02561
4. Chain-of-Thought: Wei et al. (2022) - arXiv:2201.11903
5. Multi-Agent Systems: LangGraph, CrewAI, AutoGen documentation

### Worldbuilding Theory
1. Brandon Sanderson's Laws of Magic
2. Sanderson's BYU Writing Lectures
3. Writing Excuses podcast episodes
4. Tolkien's worldbuilding philosophy

### Industry Research
1. Sudowrite architecture and multi-model stack
2. NovelAI custom models and context management
3. AI Dungeon conversational generation
4. RAG quality research (Google Cloud, AWS, Databricks)

**Full research reports available**:
- `/docs/SANDERSON_WORLDBUILDING_RESEARCH.md`
- Research agent outputs (included with this plan)

---

## Appendix B: Code Templates

### Chain-of-Thought Prompt Template
```javascript
const cotPrompt = `Before generating, think through your approach:

<reasoning>
1. Core concept: What makes this world unique?
2. Implications: How does it affect society/economy/daily life?
3. Conflicts: What tensions naturally arise?
4. Specificity: What concrete details make it real?
5. Originality: How to avoid clich√©s?
</reasoning>

Now generate following these principles...`;
```

### Clich√© Detection Function
```javascript
export function detectCliches(text) {
  const patterns = {
    naming: [/The\s+(?:Ancient|Dark|Forbidden)\s+\w+/gi],
    descriptors: [/\b(?:ancient|mystical|mysterious)\b/gi],
    tropes: [/chosen\s+one/gi]
  };

  let score = 10;
  const detected = [];

  Object.entries(patterns).forEach(([category, regexes]) => {
    regexes.forEach(regex => {
      const matches = text.match(regex);
      if (matches) {
        score -= matches.length * 0.5;
        detected.push({ category, matches });
      }
    });
  });

  return {
    score: Math.max(0, score),
    detected,
    shouldRegenerate: score < 7.0
  };
}
```

### Genre Configuration
```javascript
const genreConfigs = {
  fantasy: {
    weights: { originality: 0.35, specificity: 0.25 },
    clich√©s: ['chosen one', 'ancient evil'],
    focus: ['magic systems', 'cultures']
  },
  scifi: {
    weights: { implications: 0.35, specificity: 0.30 },
    clich√©s: ['evil AI', 'warp drive'],
    focus: ['technology', 'future societies']
  }
};
```

### Multi-Agent Workflow
```javascript
import { StateGraph, END } from "@langchain/langgraph";

const workflow = new StateGraph({ channels: graphState });

workflow.addNode("originalityCritic", originalityCritic);
workflow.addNode("specificityCritic", specificityCritic);
workflow.addNode("calculateScore", calculateScore);
workflow.addNode("reviser", reviser);

workflow.addEdge("START", "originalityCritic");
workflow.addEdge("originalityCritic", "calculateScore");
workflow.addEdge("calculateScore", "reviser");
workflow.addEdge("reviser", END);

const graph = workflow.compile();
```

---

## Appendix C: Quality Evaluation Rubric

### Scoring Guide (0-10 scale)

**9-10: Master-Crafted** (Sanderson/Tolkien level)
- Zero clich√©s, completely original
- Dense with specific details (numbers, names, materials)
- Deep societal implications across 5+ domains
- Perfect internal consistency
- 6+ limitations for systems (Sanderson's 2nd Law)

**8-9: Excellent** (Professional quality)
- Rare clich√©s (<5%)
- High specificity (30+ concrete details)
- Clear implications across 3-4 domains
- Strong internal consistency
- 4+ limitations for systems

**7-8: Very Good** (Phase 1 target)
- Few clich√©s (10-15%)
- Good specificity (20+ concrete details)
- Implications visible in 2-3 domains
- Mostly consistent
- 2-3 limitations for systems

**6-7: Good** (Current baseline)
- Some clich√©s (20-30%)
- Moderate specificity (10-15 details)
- Basic implications
- Some inconsistencies
- 1-2 limitations

**5-6: Fair** (Below target)
- Many clich√©s (30-40%)
- Vague descriptions
- Minimal implications
- Noticeable inconsistencies
- Unclear limitations

**0-5: Poor** (AI slop)
- Clich√©-ridden (40%+)
- Generic descriptions
- No societal depth
- Major contradictions
- No meaningful limitations

---

## Document Version History

- **v1.0** (2025-10-13): Initial implementation plan
  - 3 phases defined (Quick Wins, Multi-Agent, RAG)
  - Technical architecture specified
  - Cost-benefit analysis included
  - Success metrics established

---

**End of Implementation Plan**

This document serves as the master roadmap for transforming the AI worldbuilding system into a Replit-like tool with Sanderson/Tolkien-level quality. Review, adjust as needed, and execute phase by phase with clear decision points along the way.